# Multi Layer Perceptron

An implementation of multi layer perceptron in python from scratch.

The neural network model can be changed according to the problem.

## Example Problem 
Implementing a MLP algorithm for f(x, y) = x^2 + y^2 function 

## Data Set

Train and Test elements consist of random decimal x and y values in the range 0 - 2 

## Neural Network Model

![network](/graphs/network.png)

## Train Loss and Test Graphs

Train and test graphs, which result from different learning and error rates, are shown below.

The graphic on the left shows the error change during training.

The graphic on the right shows the overlap between predicted values and actual values.

### Sample 1
* Error Rate : 0.1
* Learning Rate : 0.1
* Iteration : 2100

![](/graphs/1.png)

### Sample 2
* Error Rate : 0.1
* Learning Rate : 0.05
* Iteration : 4200

![](/graphs/2.png)
### Sample 3
* Error Rate : 0.1
* Learning Rate : 0.01
* Iteration : 10200

![](/graphs/3.png)
### Sample 4
* Error Rate : 0.05
* Learning Rate : 0.1
* Iteration : 2200

![](/graphs/4.png)
### Sample 5
* Error Rate : 0.05
* Learning Rate : 0.05
* Iteration : 3900

![](/graphs/5.png)
### Sample 6
* Error Rate : 0.05
* Learning Rate : 0.01
* Iteration : 15900

![](/graphs/6.png)
### Sample 7
* Error Rate : 0.01
* Learning Rate : 0.1
* Iteration : 12300

![](/graphs/7.png)
### Sample 8
* Error Rate : 0.01
* Learning Rate : 0.05
* Iteration : 6000

![](/graphs/8.png)
### Sample 9
* Error Rate : 0.01
* Learning Rate : 0.01
* Iteration : 26100

![](/graphs/9.png)
